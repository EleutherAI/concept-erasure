{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/mnt/ssd-2/hf_cache/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa2899708924f6bacc240ba31d8e8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"universal_dependencies\", \"en_gum\")\n",
    "\n",
    "# DO NOT SORT: the order actually matters since we index into the list\n",
    "upos_tags = [\n",
    "    \"NOUN\",\n",
    "    \"PUNCT\",\n",
    "    \"ADP\",\n",
    "    \"NUM\",\n",
    "    \"SYM\",\n",
    "    \"SCONJ\",\n",
    "    \"ADJ\",\n",
    "    \"PART\",\n",
    "    \"DET\",\n",
    "    \"CCONJ\",\n",
    "    \"PROPN\",\n",
    "    \"PRON\",\n",
    "    \"X\",\n",
    "    \"_\",\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\n",
    "    \"VERB\",\n",
    "    \"AUX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = dataset['train']['tokens']\n",
    "test_tokens = dataset['test']['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import ConceptScrubber\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, tokens, raw_labels):\n",
    "    # receive a list of tokens and return a list of token ids as well as a mapping from token ids to tokens\n",
    "    token_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for original, label in zip(tokens, raw_labels):\n",
    "        ids = tokenizer.encode(original, add_special_tokens=False)\n",
    "        labels.extend([label] * len(ids))\n",
    "        token_ids.extend(ids)\n",
    "\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, sentences: list[list[str]], labels: list[list[str]]):\n",
    "    losses = []\n",
    "    scrubber = ConceptScrubber(model, y_dim=len(upos_tags), cov_type=\"eye\")\n",
    "    \n",
    "    for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        label = F.one_hot(\n",
    "            torch.tensor(labels).to(model.device),\n",
    "            len(upos_tags),\n",
    "        )\n",
    "\n",
    "        with scrubber.record(label):\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "    \n",
    "    print(f\"{torch.stack(losses).isfinite().float().mean()} of losses are finite\")\n",
    "    return scrubber, torch.stack(losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset red_pajama-data-1_t-sample (/mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n",
      "Loading cached shuffled indices for dataset at /mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039/cache-f48a7eaa185823dd.arrow\n"
     ]
    }
   ],
   "source": [
    "test_set = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train\"\n",
    ").shuffle(\n",
    "    seed=42\n",
    ").select(\n",
    "    range(5000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618f491f840a472d8fa714abcf9950de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f92150c3a434bb9b4f4426380bd32ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d06000cced49a1b88c2360db6abc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a053e6919e40a09aff54d23d98ff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7562c860c538482ca678f67970057ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa20b0984243749bfd6f4815bae7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aecdb7b65d345bbbb0a65e1b910af36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd7e350387240759330d90d1a87afe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1520344604fc4023a4ff2a2b5e4dd5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61f3fd70cc74acc9e1448ad48db8317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404614799c334325a6184e2ab0caafb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2aa51ab79d489783d1506780ae3711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"huggyllama/llama-13b\", device_map={\"\": \"cuda:0\"}, torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [04:48<00:00, 14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941684603691101 of losses are finite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrubber, clean_loss = encode(\n",
    "    model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean loss: 1.87: 100%|██████████| 5000/5000 [11:16<00:00,  7.39it/s]  \n"
     ]
    }
   ],
   "source": [
    "# d = model.config.hidden_size\n",
    "scrubber.clear_x()\n",
    "\n",
    "with scrubber.record() as scrubber, torch.no_grad():\n",
    "    clean_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = tokenizer.encode(\n",
    "            record['text'],\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        clean_losses.append(model(x, labels=x).loss)\n",
    "        pbar.set_description(\n",
    "            f\"Clean loss: {torch.stack(clean_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random loss: 1.88: 100%|██████████| 5000/5000 [13:46<00:00,  6.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = tokenizer.encode(\n",
    "            record['text'],\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with scrubber.random_scrub():\n",
    "            random_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Random loss: {torch.stack(random_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrubbed loss: 8.55: 100%|██████████| 5000/5000 [11:19<00:00,  7.35it/s]  \n"
     ]
    }
   ],
   "source": [
    "with scrubber.scrub() as scrubber, torch.no_grad():\n",
    "    scrubbed_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = tokenizer.encode(\n",
    "            record['text'],\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        scrubbed_losses.append(model(x, labels=x).loss)\n",
    "        pbar.set_description(\n",
    "            f\"Scrubbed loss: {torch.stack(scrubbed_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1662, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8471, device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.2455, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(scrubbed_losses).nanmean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
