{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/mnt/ssd-2/hf_cache/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cb67b1178b4f90a63d09e6734b040d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"universal_dependencies\", \"en_gum\")\n",
    "\n",
    "# DO NOT SORT: the order actually matters since we index into the list\n",
    "upos_tags = [\n",
    "    \"NOUN\",\n",
    "    \"PUNCT\",\n",
    "    \"ADP\",\n",
    "    \"NUM\",\n",
    "    \"SYM\",\n",
    "    \"SCONJ\",\n",
    "    \"ADJ\",\n",
    "    \"PART\",\n",
    "    \"DET\",\n",
    "    \"CCONJ\",\n",
    "    \"PROPN\",\n",
    "    \"PRON\",\n",
    "    \"X\",\n",
    "    \"_\",\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\n",
    "    \"VERB\",\n",
    "    \"AUX\",\n",
    "]\n",
    "tag_to_id = {tag: i for i, tag in enumerate(upos_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import ConceptEraser\n",
    "from contextlib import contextmanager\n",
    "from torch import nn, Tensor\n",
    "from transformers import BertModel, RobertaModel, LlamaModel\n",
    "from typing import Sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "class ConceptScrubber:\n",
    "    label: Tensor | None\n",
    "    mask: Tensor | None\n",
    "\n",
    "    def __init__(self, model, y_dim: int = 1, rank: int | None = None):\n",
    "        d_model = model.config.hidden_size\n",
    "        base = model.base_model\n",
    "\n",
    "        if isinstance(base, LlamaModel):\n",
    "            layers = base.layers\n",
    "        elif isinstance(base, (BertModel, RobertaModel)):\n",
    "            layers = base.encoder.layer\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type {type(base)}\")\n",
    "\n",
    "        self.x_dim = d_model\n",
    "        self.y_dim = y_dim\n",
    "\n",
    "        self.label = None\n",
    "        self.mask = None\n",
    "        self.erasers = {\n",
    "            layer: ConceptEraser(\n",
    "                d_model, y_dim, device=model.device, dtype=model.dtype, rank=rank\n",
    "            )\n",
    "            for layer in layers\n",
    "        }\n",
    "\n",
    "    @contextmanager\n",
    "    def record(self):\n",
    "        # Called after every layer forward pass\n",
    "        def record_hook(layer, _, output):\n",
    "            assert self.label is not None\n",
    "            x, *extras = output\n",
    "\n",
    "            self.erasers[layer].update(\n",
    "                x[self.mask] if self.mask is not None else x, self.label\n",
    "            )\n",
    "            return (x, *extras)\n",
    "\n",
    "        handles = {\n",
    "            layer: layer.register_forward_hook(record_hook)\n",
    "            for layer in self.erasers\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            yield self\n",
    "        finally:\n",
    "            # Make sure to remove the hooks even if an exception is raised\n",
    "            for handle in handles.values():\n",
    "                handle.remove()\n",
    "\n",
    "    @contextmanager\n",
    "    def scrub(self, layer_indices: Sequence[int] = ()):\n",
    "        # Called after every layer forward pass\n",
    "        def apply_hook(layer, _, output):\n",
    "            x, *extras = output\n",
    "\n",
    "            if self.mask is not None:\n",
    "                x[self.mask] = self.erasers[layer](x[self.mask])\n",
    "            else:\n",
    "                x = self.erasers[layer](x)\n",
    "\n",
    "            return (x, *extras)\n",
    "        \n",
    "        if layer_indices:\n",
    "            layer_list = list(self.erasers.keys())\n",
    "            layers = [layer_list[i] for i in layer_indices]\n",
    "        else:\n",
    "            layers = self.erasers.keys()\n",
    "\n",
    "        handles = {\n",
    "            layer: layer.register_forward_hook(apply_hook)\n",
    "            for layer in layers\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            yield self\n",
    "        finally:\n",
    "            # Make sure to remove the hooks even if an exception is raised\n",
    "            for handle in handles.values():\n",
    "                handle.remove()\n",
    "    \n",
    "    @contextmanager\n",
    "    def random_scrub(self, layer_indices: Sequence[int] = ()):\n",
    "        eraser = next(iter(self.erasers.values()))\n",
    "        u = nn.init.orthogonal_(torch.empty_like(eraser.u))\n",
    "\n",
    "        # Called after every layer forward pass\n",
    "        def apply_hook(layer, _, output):\n",
    "            x, *extras = output\n",
    "            mean = self.erasers[layer].mean_x\n",
    "\n",
    "            if self.mask is not None:\n",
    "                delta = (x[self.mask] - mean) @ u @ u.mT\n",
    "                x[self.mask] -= delta\n",
    "            else:\n",
    "                delta = (x - mean) @ u @ u.mT\n",
    "                x -= x @ u @ u.T\n",
    "\n",
    "            return (x, *extras)\n",
    "        \n",
    "        if layer_indices:\n",
    "            layer_list = list(self.erasers.keys())\n",
    "            layers = [layer_list[i] for i in layer_indices]\n",
    "        else:\n",
    "            layers = self.erasers.keys()\n",
    "\n",
    "        handles = {\n",
    "            layer: layer.register_forward_hook(apply_hook)\n",
    "            for layer in layers\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            yield self\n",
    "        finally:\n",
    "            # Make sure to remove the hooks even if an exception is raised\n",
    "            for handle in handles.values():\n",
    "                handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = dataset['train']['tokens']\n",
    "test_tokens = dataset['test']['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, tokens, raw_labels):\n",
    "    # receive a list of tokens and return a list of token ids as well as a mapping from token ids to tokens\n",
    "    token_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for original, label in zip(tokens, raw_labels):\n",
    "        ids = tokenizer.encode(original, add_special_tokens=False)\n",
    "        labels.extend([label] * len(ids))\n",
    "        token_ids.extend(ids)\n",
    "\n",
    "    token_ids = [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id]\n",
    "    labels = [13] + labels + [13]\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, sentences: list[list[str]], labels: list[list[str]]):\n",
    "    losses = []\n",
    "    \n",
    "    with ConceptScrubber(model, y_dim=len(upos_tags)).record() as scrubber:\n",
    "        for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "            ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "            x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "            scrubber.label = F.one_hot(\n",
    "                torch.tensor(labels).to(model.device),\n",
    "                len(upos_tags),\n",
    "            )\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "    \n",
    "    return scrubber, torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [00:56<00:00, 75.80it/s]\n"
     ]
    }
   ],
   "source": [
    "scrubber, clean_loss = encode(\n",
    "    model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [00:42<00:00, 101.77it/s]\n"
     ]
    }
   ],
   "source": [
    "with scrubber.scrub() as scrubber, torch.no_grad():\n",
    "    it = zip(train_tokens, dataset['train']['upos'])\n",
    "    preds = []\n",
    "\n",
    "    for sentence, label_seq in tqdm(it, total=len(train_tokens)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        scrubber.label = F.one_hot(\n",
    "            torch.tensor(labels).to(model.device),\n",
    "            len(upos_tags),\n",
    "        )\n",
    "        preds.append(model(x, labels=x).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [00:40<00:00, 106.06it/s]\n"
     ]
    }
   ],
   "source": [
    "with scrubber.random_scrub() as scrubber, torch.no_grad():\n",
    "    it = zip(train_tokens, dataset['train']['upos'])\n",
    "    random_preds = []\n",
    "\n",
    "    for sentence, label_seq in tqdm(it, total=len(train_tokens)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        scrubber.label = F.one_hot(\n",
    "            torch.tensor(labels).to(model.device),\n",
    "            len(upos_tags),\n",
    "        )\n",
    "        random_preds.append(model(x, labels=x).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.000048875808716"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8951, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(random_preds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2724, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(preds).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
