{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/mnt/ssd-2/hf_cache/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655805951fb04984a5eda112e55f7873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"universal_dependencies\", \"en_gum\")\n",
    "\n",
    "# DO NOT SORT: the order actually matters since we index into the list\n",
    "upos_tags = [\n",
    "    \"NOUN\",\n",
    "    \"PUNCT\",\n",
    "    \"ADP\",\n",
    "    \"NUM\",\n",
    "    \"SYM\",\n",
    "    \"SCONJ\",\n",
    "    \"ADJ\",\n",
    "    \"PART\",\n",
    "    \"DET\",\n",
    "    \"CCONJ\",\n",
    "    \"PROPN\",\n",
    "    \"PRON\",\n",
    "    \"X\",\n",
    "    \"_\",\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\n",
    "    \"VERB\",\n",
    "    \"AUX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = dataset['train']['tokens']\n",
    "test_tokens = dataset['test']['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import ConceptScrubber\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, tokens, raw_labels):\n",
    "    # receive a list of tokens and return a list of token ids as well as a mapping from token ids to tokens\n",
    "    token_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for original, label in zip(tokens, raw_labels):\n",
    "        ids = tokenizer.encode(original, add_special_tokens=False)\n",
    "        labels.extend([label] * len(ids))\n",
    "        token_ids.extend(ids)\n",
    "\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, sentences: list[list[str]], labels: list[list[str]]):\n",
    "    losses = []\n",
    "    scrubber = ConceptScrubber(model, y_dim=len(upos_tags))\n",
    "    label_list = []\n",
    "\n",
    "    for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        label = F.one_hot(\n",
    "            torch.tensor(labels).to(model.device),\n",
    "            len(upos_tags),\n",
    "        )\n",
    "        with scrubber.record(model, label):\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "    \n",
    "    print(f\"{torch.stack(losses).isfinite().float().mean()} of losses are finite\")\n",
    "    return scrubber, torch.stack(losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sanity_check(\n",
    "    scrubber, model, tokenizer, sentences: list[list[str]], labels: list[list[str]]\n",
    "):\n",
    "    losses = []\n",
    "    model.float()\n",
    "\n",
    "    hidden_lists = [[] for _ in range(model.config.num_hidden_layers)]\n",
    "    label_list = []\n",
    "    \n",
    "    for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        with scrubber.scrub(model, return_hiddens=True) as layer_hiddens:\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        for hiddens, buf in zip(layer_hiddens, hidden_lists):\n",
    "            buf.append(hiddens)\n",
    "\n",
    "        label_list.extend(labels)\n",
    "    \n",
    "    print(f\"{torch.stack(losses).isfinite().float().mean()} of losses are finite\")\n",
    "    return hidden_lists, label_list, torch.stack(losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "@torch.no_grad()\n",
    "def fit_sequential(\n",
    "    model, tokenizer, sentences: list[list[str]], labels: list[list[str]]\n",
    "):\n",
    "    iter_losses = []\n",
    "    scrubber = ConceptScrubber(model, y_dim=len(upos_tags), affine=True, cov_type=\"full\")\n",
    "    model.float()\n",
    "\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        losses = []\n",
    "        scrub_layers = tuple(range(i))\n",
    "\n",
    "        for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "            ids, y = tokenize(tokenizer, sentence, label_seq)\n",
    "            x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "            label = F.one_hot(\n",
    "                torch.tensor(y).to(model.device),\n",
    "                len(upos_tags),\n",
    "            )\n",
    "            with (\n",
    "                scrubber.scrub(model, layer_indices=scrub_layers) if scrub_layers else nullcontext(),\n",
    "                scrubber.record(model, label=label, layer_indices=(i,))\n",
    "            ):\n",
    "                losses.append(model(x, labels=x).loss)\n",
    "        \n",
    "        iter_losses.append(\n",
    "            torch.stack(losses).nanmean()\n",
    "        )\n",
    "        \n",
    "    return scrubber, iter_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-1.4b\",\n",
    "    device_map={\"\": \"cuda:0\"},\n",
    "    #load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset red_pajama-data-1_t-sample (/mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n",
      "Loading cached shuffled indices for dataset at /mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039/cache-f48a7eaa185823dd.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039/cache-62a712b27c17a306_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "from concept_erasure import chunk_and_tokenize\n",
    "from datasets import Dataset\n",
    "\n",
    "test_set = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train\"\n",
    ")\n",
    "assert isinstance(test_set, Dataset)\n",
    "\n",
    "test_set = test_set.shuffle(\n",
    "    seed=42\n",
    ").select(\n",
    "    range(2048)\n",
    ")\n",
    "test_set, nats_per_bpb = chunk_and_tokenize(test_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.zeros(50277)\n",
    "\n",
    "for record in test_set:\n",
    "    counts[record[\"input_ids\"]] += 1\n",
    "\n",
    "probs = counts / counts.sum()\n",
    "H = -(probs * probs.add(1e-8).log()).sum() * nats_per_bpb\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "def robust_pinvh(A: Tensor) -> Tensor:\n",
    "    \"\"\"Robust Moore-Penrose pseudoinverse for Hermitian matrices.\"\"\"\n",
    "    try:\n",
    "        L, Q = torch.linalg.eigh(A)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        return torch.pinverse(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [02:22<00:00, 30.05it/s]\n",
      "100%|██████████| 4287/4287 [02:24<00:00, 29.73it/s]\n",
      "100%|██████████| 4287/4287 [02:25<00:00, 29.48it/s]\n",
      "100%|██████████| 4287/4287 [02:25<00:00, 29.53it/s]\n",
      "100%|██████████| 4287/4287 [02:25<00:00, 29.50it/s]\n",
      "100%|██████████| 4287/4287 [02:26<00:00, 29.23it/s]\n",
      "100%|██████████| 4287/4287 [02:27<00:00, 29.16it/s]\n",
      "100%|██████████| 4287/4287 [02:28<00:00, 28.97it/s]\n",
      "100%|██████████| 4287/4287 [02:28<00:00, 28.78it/s]\n",
      "  0%|          | 0/4287 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at layer 9: Non-finite values in covariance matrix\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Non-finite values in covariance matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mwith\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m         scrubber\u001b[39m.\u001b[39mscrub(model, layer_indices\u001b[39m=\u001b[39mscrub_layers) \u001b[39mif\u001b[39;00m scrub_layers \u001b[39melse\u001b[39;00m nullcontext(),\n\u001b[1;32m     23\u001b[0m         scrubber\u001b[39m.\u001b[39mrecord(model, label\u001b[39m=\u001b[39mlabel, layer_indices\u001b[39m=\u001b[39m(i,))\n\u001b[1;32m     24\u001b[0m     ):\n\u001b[0;32m---> 25\u001b[0m         losses\u001b[39m.\u001b[39mappend(model(x, labels\u001b[39m=\u001b[39;49mx)\u001b[39m.\u001b[39mloss)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError at layer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:662\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    660\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 662\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    663\u001b[0m     input_ids,\n\u001b[1;32m    664\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    665\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    666\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    667\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    668\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    669\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    670\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    671\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    672\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    676\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:553\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    545\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    546\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    547\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m         head_mask[i],\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    554\u001b[0m         hidden_states,\n\u001b[1;32m    555\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    556\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    557\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    558\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    559\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    560\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    563\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_scrubber.py:107\u001b[0m, in \u001b[0;36mConceptScrubber.scrub.<locals>.apply_hook\u001b[0;34m(_, args, layer_idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m x, \u001b[39m*\u001b[39mextras \u001b[39m=\u001b[39m args\n\u001b[1;32m    106\u001b[0m eraser \u001b[39m=\u001b[39m assert_type(ConceptEraser, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merasers[layer_idx])\n\u001b[0;32m--> 107\u001b[0m x_ \u001b[39m=\u001b[39m eraser(x)\u001b[39m.\u001b[39mtype_as(x) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dry_run \u001b[39melse\u001b[39;00m x\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m return_hiddens:\n\u001b[1;32m    109\u001b[0m     hiddens\u001b[39m.\u001b[39mappend(x_)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:103\u001b[0m, in \u001b[0;36mConceptEraser.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m d\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m (x \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_x) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mP\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_x\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mP\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:206\u001b[0m, in \u001b[0;36mConceptEraser.P\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[39m# We only want to erase the highest energy part of the subspace\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         u, _, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msvd_lowrank(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxcov, q\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank)\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_P \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_for_subspace(u)\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_P\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:121\u001b[0m, in \u001b[0;36mConceptEraser.proj_for_subspace\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    119\u001b[0m sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_x\u001b[39m.\u001b[39mdiag_embed() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiag\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_x\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sigma\u001b[39m.\u001b[39misfinite()\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNon-finite values in covariance matrix\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Q\u001b[39m.\u001b[39misfinite()\u001b[39m.\u001b[39mall():\n\u001b[1;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNon-finite values in projection matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Non-finite values in covariance matrix"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sentences = train_tokens\n",
    "    labels = dataset['train']['upos']\n",
    "\n",
    "    iter_losses = []\n",
    "    scrubber = ConceptScrubber(model, y_dim=len(upos_tags), affine=True, cov_type=\"full\")\n",
    "\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        losses = []\n",
    "        scrub_layers = tuple(range(i))\n",
    "\n",
    "        for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "            ids, y = tokenize(tokenizer, sentence, label_seq)\n",
    "            x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "            label = F.one_hot(\n",
    "                torch.tensor(y).to(model.device),\n",
    "                len(upos_tags),\n",
    "            )\n",
    "            try:\n",
    "                with (\n",
    "                    scrubber.scrub(model, layer_indices=scrub_layers) if scrub_layers else nullcontext(),\n",
    "                    scrubber.record(model, label=label, layer_indices=(i,))\n",
    "                ):\n",
    "                    losses.append(model(x, labels=x).loss)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at layer {i}: {e}\")\n",
    "                raise\n",
    "        \n",
    "        iter_losses.append(\n",
    "            torch.stack(losses).nanmean()\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0510e+24,  4.0682e+23,  1.1058e+24,  ..., -1.3804e+24,\n",
       "          1.0396e+24, -1.3749e+24],\n",
       "        [ 4.0682e+23,  1.5746e+23,  4.2801e+23,  ..., -5.3430e+23,\n",
       "          4.0238e+23, -5.3218e+23],\n",
       "        [ 1.1058e+24,  4.2801e+23,  1.1634e+24,  ..., -1.4523e+24,\n",
       "          1.0937e+24, -1.4465e+24],\n",
       "        ...,\n",
       "        [-1.3804e+24, -5.3430e+23, -1.4523e+24,  ...,  1.8130e+24,\n",
       "         -1.3653e+24,  1.8058e+24],\n",
       "        [ 1.0396e+24,  4.0238e+23,  1.0937e+24,  ..., -1.3653e+24,\n",
       "          1.0282e+24, -1.3599e+24],\n",
       "        [-1.3749e+24, -5.3218e+23, -1.4465e+24,  ...,  1.8058e+24,\n",
       "         -1.3599e+24,  1.7986e+24]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubber.erasers[6].cov_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [01:51<00:00, 38.39it/s]\n",
      "100%|██████████| 4287/4287 [01:52<00:00, 37.94it/s]\n",
      "100%|██████████| 4287/4287 [01:54<00:00, 37.39it/s]\n",
      "100%|██████████| 4287/4287 [01:54<00:00, 37.31it/s]\n",
      "100%|██████████| 4287/4287 [01:56<00:00, 36.95it/s]\n",
      "100%|██████████| 4287/4287 [01:56<00:00, 36.67it/s]\n",
      "100%|██████████| 4287/4287 [01:58<00:00, 36.24it/s]\n",
      "100%|██████████| 4287/4287 [02:00<00:00, 35.62it/s]\n",
      "  0%|          | 0/4287 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2032).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scrubber, losses \u001b[39m=\u001b[39m fit_sequential(\n\u001b[1;32m      2\u001b[0m     model, tokenizer, train_tokens, dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mupos\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      3\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mfit_sequential\u001b[0;34m(model, tokenizer, sentences, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m         label \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(\n\u001b[1;32m     20\u001b[0m             torch\u001b[39m.\u001b[39mtensor(y)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     21\u001b[0m             \u001b[39mlen\u001b[39m(upos_tags),\n\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m         \u001b[39mwith\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m             scrubber\u001b[39m.\u001b[39mscrub(model, layer_indices\u001b[39m=\u001b[39mscrub_layers) \u001b[39mif\u001b[39;00m scrub_layers \u001b[39melse\u001b[39;00m nullcontext(),\n\u001b[1;32m     25\u001b[0m             scrubber\u001b[39m.\u001b[39mrecord(model, label\u001b[39m=\u001b[39mlabel, layer_indices\u001b[39m=\u001b[39m(i,))\n\u001b[1;32m     26\u001b[0m         ):\n\u001b[0;32m---> 27\u001b[0m             losses\u001b[39m.\u001b[39mappend(model(x, labels\u001b[39m=\u001b[39;49mx)\u001b[39m.\u001b[39mloss)\n\u001b[1;32m     29\u001b[0m     iter_losses\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     30\u001b[0m         torch\u001b[39m.\u001b[39mstack(losses)\u001b[39m.\u001b[39mnanmean()\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m scrubber, iter_losses\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:662\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    660\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 662\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    663\u001b[0m     input_ids,\n\u001b[1;32m    664\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    665\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    666\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    667\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    668\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    669\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    670\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    671\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    672\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    676\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:553\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    545\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    546\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    547\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m         head_mask[i],\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    554\u001b[0m         hidden_states,\n\u001b[1;32m    555\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    556\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    557\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    558\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    559\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    560\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    563\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_scrubber.py:106\u001b[0m, in \u001b[0;36mConceptScrubber.scrub.<locals>.apply_hook\u001b[0;34m(_, args, layer_idx)\u001b[0m\n\u001b[1;32m    104\u001b[0m x, \u001b[39m*\u001b[39mextras \u001b[39m=\u001b[39m args\n\u001b[1;32m    105\u001b[0m eraser \u001b[39m=\u001b[39m assert_type(ConceptEraser, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merasers[layer_idx])\n\u001b[0;32m--> 106\u001b[0m x_ \u001b[39m=\u001b[39m eraser(x) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dry_run \u001b[39melse\u001b[39;00m x\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m return_hiddens:\n\u001b[1;32m    108\u001b[0m     hiddens\u001b[39m.\u001b[39mappend(x_)\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:103\u001b[0m, in \u001b[0;36mConceptEraser.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m d\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m (x \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_x) \u001b[39m@\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mP\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_x\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mP\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:188\u001b[0m, in \u001b[0;36mConceptEraser.P\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[39m# We only want to erase the highest energy part of the subspace\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         u, _, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msvd_lowrank(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxcov, q\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank)\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_P \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_for_subspace(u)\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_P\n",
      "File \u001b[0;32m/mnt/ssd-2/nora/concept-erasure/concept_erasure/concept_eraser.py:121\u001b[0m, in \u001b[0;36mConceptEraser.proj_for_subspace\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m# Adjust Q to account for the covariance of X\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_x\u001b[39m.\u001b[39mdiag_embed() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiag\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_x\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m sigma \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mpinv(Q \u001b[39m@\u001b[39;49m sigma \u001b[39m@\u001b[39;49m Q, hermitian\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 2032)."
     ]
    }
   ],
   "source": [
    "scrubber, losses = fit_sequential(\n",
    "    model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [01:08<00:00, 62.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9937019348144531 of losses are finite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, loss = sanity_check(\n",
    "    scrubber, model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import (\n",
    "    binary_cross_entropy_with_logits as bce_with_logits,\n",
    ")\n",
    "from torch.nn.functional import (\n",
    "    cross_entropy,\n",
    ")\n",
    "\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    \"\"\"Linear classifier trained with supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_classes: int = 2,\n",
    "        device: str | torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(\n",
    "            input_dim, num_classes if num_classes > 2 else 1, device=device, dtype=dtype\n",
    "        )\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear(x).squeeze(-1)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def fit(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor,\n",
    "        *,\n",
    "        l2_penalty: float = 0.0,\n",
    "        max_iter: int = 10_000,\n",
    "    ) -> float:\n",
    "        \"\"\"Fits the model to the input data using L-BFGS with L2 regularization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, D), where N is the number of samples and D is\n",
    "                the input dimension.\n",
    "            y: Target tensor of shape (N,) for binary classification or (N, C) for\n",
    "                multiclass classification, where C is the number of classes.\n",
    "            l2_penalty: L2 regularization strength.\n",
    "            max_iter: Maximum number of iterations for the L-BFGS optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Final value of the loss function after optimization.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            self.parameters(),\n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            max_iter=max_iter,\n",
    "        )\n",
    "\n",
    "        num_classes = self.linear.out_features\n",
    "        loss_fn = bce_with_logits if num_classes == 1 else cross_entropy\n",
    "        loss = torch.inf\n",
    "        y = y.to(\n",
    "            torch.get_default_dtype() if num_classes == 1 else torch.long,\n",
    "        )\n",
    "\n",
    "        def closure():\n",
    "            nonlocal loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss function\n",
    "            logits = self(x).squeeze(-1)\n",
    "            loss = loss_fn(logits, y)\n",
    "            if l2_penalty:\n",
    "                reg_loss = loss + l2_penalty * self.linear.weight.square().sum()\n",
    "            else:\n",
    "                reg_loss = loss\n",
    "\n",
    "            reg_loss.backward()\n",
    "            return float(reg_loss)\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        return float(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([h.squeeze(0) for h in x[-1]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(y, dtype=torch.long, device=X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(\n",
    "    x[-1][0].shape[-1], num_classes=len(upos_tags), device=X.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "Y_1h = F.one_hot(Y, len(upos_tags)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3862e-09,  7.2934e-09,  9.7994e-09,  ...,  9.3750e-11,\n",
       "         -6.8982e-09, -8.0569e-09],\n",
       "        [ 4.4910e-11,  1.6019e-08,  1.8144e-09,  ..., -4.6749e-10,\n",
       "         -1.4405e-08, -5.7260e-09],\n",
       "        [ 3.5569e-09, -1.4820e-09, -6.4671e-10,  ...,  4.8699e-11,\n",
       "         -8.9820e-10,  5.1018e-09],\n",
       "        ...,\n",
       "        [-6.5748e-09, -3.5928e-11,  5.6227e-09,  ..., -3.3079e-10,\n",
       "          2.6587e-09, -2.1153e-09],\n",
       "        [-4.3473e-09,  1.2754e-09, -1.2665e-09,  ...,  3.5226e-11,\n",
       "         -4.3114e-10,  7.1856e-11],\n",
       "        [-1.1497e-09,  6.1078e-10, -3.0011e-09,  ...,  2.9093e-10,\n",
       "          2.4790e-09,  1.1317e-09]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcov = (X - X.mean(0)).T @ (Y_1h - Y_1h.mean(0)) / (X.shape[0] - 1)\n",
    "xcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4124042987823486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4123, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.bincount(Y)\n",
    "probs = counts / counts.sum()\n",
    "H = -(probs * probs.add(1e-8).log()).sum()\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean loss: 3.11: 100%|██████████| 1216/1216 [01:42<00:00, 11.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#scrubber.clear_x()\n",
    "scrubber_adapted = deepcopy(scrubber)\n",
    "scrubber_adapted.clear_x()\n",
    "\n",
    "with scrubber_adapted.record(model), torch.no_grad():\n",
    "    clean_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        clean_losses.append(model(x, labels=x).loss)\n",
    "        pbar.set_description(\n",
    "            f\"Clean loss: {torch.stack(clean_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random loss: 3.47: 100%|██████████| 1216/1216 [01:44<00:00, 11.60it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_losses = []\n",
    "    model = model.float()\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber_adapted.random_scrub(model):\n",
    "            random_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Random loss: {torch.stack(random_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random loss: 3.47: 100%|██████████| 1216/1216 [01:45<00:00, 11.57it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_losses = []\n",
    "    model = model.float()\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber.random_scrub(model):\n",
    "            random_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Random loss: {torch.stack(random_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubber.erasers[1].n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrubbed loss: 9.20: 100%|██████████| 1216/1216 [01:45<00:00, 11.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from concept_erasure import ConceptEraser\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    scrubbed_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber_adapted.scrub(model):\n",
    "            scrubbed_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Scrubbed loss: {torch.stack(scrubbed_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33238209937056146"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats_per_bpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1662, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8471, device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.2455, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(scrubbed_losses).nanmean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
