{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/mnt/ssd-2/hf_cache/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f222fb11843e432183fc9be16570d2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"universal_dependencies\", \"en_gum\")\n",
    "\n",
    "# DO NOT SORT: the order actually matters since we index into the list\n",
    "upos_tags = [\n",
    "    \"NOUN\",\n",
    "    \"PUNCT\",\n",
    "    \"ADP\",\n",
    "    \"NUM\",\n",
    "    \"SYM\",\n",
    "    \"SCONJ\",\n",
    "    \"ADJ\",\n",
    "    \"PART\",\n",
    "    \"DET\",\n",
    "    \"CCONJ\",\n",
    "    \"PROPN\",\n",
    "    \"PRON\",\n",
    "    \"X\",\n",
    "    \"_\",\n",
    "    \"ADV\",\n",
    "    \"INTJ\",\n",
    "    \"VERB\",\n",
    "    \"AUX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = dataset['train']['tokens']\n",
    "test_tokens = dataset['test']['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import ConceptScrubber\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, tokens, raw_labels):\n",
    "    # receive a list of tokens and return a list of token ids as well as a mapping from token ids to tokens\n",
    "    token_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for original, label in zip(tokens, raw_labels):\n",
    "        ids = tokenizer.encode(original, add_special_tokens=False)\n",
    "        labels.extend([label] * len(ids))\n",
    "        token_ids.extend(ids)\n",
    "\n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(model, tokenizer, sentences: list[list[str]], labels: list[list[str]]):\n",
    "    losses = []\n",
    "    scrubber = ConceptScrubber(\n",
    "        model, y_dim=len(upos_tags), clip_variances=False\n",
    "    )\n",
    "    label_list = []\n",
    "    \n",
    "    for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        label = F.one_hot(\n",
    "            torch.tensor(labels).to(model.device),\n",
    "            len(upos_tags),\n",
    "        )\n",
    "        with scrubber.record(model, label):\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "    \n",
    "    print(f\"{torch.stack(losses).isfinite().float().mean()} of losses are finite\")\n",
    "    return scrubber, torch.stack(losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sanity_check(\n",
    "    scrubber, model, tokenizer, sentences: list[list[str]], labels: list[list[str]]\n",
    "):\n",
    "    losses = []\n",
    "    model.float()\n",
    "\n",
    "    hidden_lists = [[] for _ in range(model.config.num_hidden_layers)]\n",
    "    label_list = []\n",
    "    \n",
    "    for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "        ids, labels = tokenize(tokenizer, sentence, label_seq)\n",
    "        x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "        with scrubber.scrub(model, dry_run=False, return_hiddens=True) as layer_hiddens:\n",
    "            losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        for hiddens, buf in zip(layer_hiddens, hidden_lists):\n",
    "            buf.append(hiddens)\n",
    "\n",
    "        label_list.extend(labels)\n",
    "    \n",
    "    print(f\"{torch.stack(losses).isfinite().float().mean()} of losses are finite\")\n",
    "    return hidden_lists, label_list, torch.stack(losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "@torch.no_grad()\n",
    "def fit_sequential(\n",
    "    model, tokenizer, sentences: list[list[str]], labels: list[list[str]]\n",
    "):\n",
    "    iter_losses = []\n",
    "    scrubber = ConceptScrubber(model, y_dim=len(upos_tags), affine=False, cov_type=\"eye\")\n",
    "    model.float()\n",
    "\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        losses = []\n",
    "        scrub_layers = tuple(range(i))\n",
    "\n",
    "        for sentence, label_seq in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "            ids, y = tokenize(tokenizer, sentence, label_seq)\n",
    "            x = torch.tensor([ids]).to(model.device)\n",
    "\n",
    "            label = F.one_hot(\n",
    "                torch.tensor(y).to(model.device),\n",
    "                len(upos_tags),\n",
    "            )\n",
    "            with (\n",
    "                scrubber.scrub(model, layer_indices=scrub_layers) if scrub_layers else nullcontext(),\n",
    "                scrubber.record(model, label=label, layer_indices=(i,))\n",
    "            ):\n",
    "                losses.append(model(x, labels=x).loss)\n",
    "        \n",
    "        iter_losses.append(\n",
    "            torch.stack(losses).nanmean()\n",
    "        )\n",
    "        \n",
    "    return scrubber, iter_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    device_map={\"\": \"cuda:0\"},\n",
    "    #load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset red_pajama-data-1_t-sample (/mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n",
      "Loading cached shuffled indices for dataset at /mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039/cache-f48a7eaa185823dd.arrow\n",
      "Loading cached processed dataset at /mnt/ssd-2/hf_cache/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039/cache-98f532e3134323b2_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "from concept_erasure import chunk_and_tokenize\n",
    "from datasets import Dataset\n",
    "\n",
    "test_set = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train\"\n",
    ")\n",
    "assert isinstance(test_set, Dataset)\n",
    "\n",
    "test_set = test_set.shuffle(\n",
    "    seed=42\n",
    ").select(\n",
    "    range(2048)\n",
    ")\n",
    "test_set, nats_per_bpb = chunk_and_tokenize(test_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.zeros(50277)\n",
    "\n",
    "for record in test_set:\n",
    "    counts[record[\"input_ids\"]] += 1\n",
    "\n",
    "probs = counts / counts.sum()\n",
    "H = -(probs * probs.add(1e-8).log()).sum() * nats_per_bpb\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [01:09<00:00, 61.68it/s]\n",
      "100%|██████████| 4287/4287 [01:09<00:00, 61.93it/s]\n",
      "100%|██████████| 4287/4287 [01:08<00:00, 62.24it/s]\n",
      "100%|██████████| 4287/4287 [01:09<00:00, 61.44it/s]\n",
      "100%|██████████| 4287/4287 [01:10<00:00, 61.06it/s]\n",
      "100%|██████████| 4287/4287 [01:10<00:00, 60.75it/s]\n",
      "100%|██████████| 4287/4287 [01:10<00:00, 60.59it/s]\n",
      "100%|██████████| 4287/4287 [01:11<00:00, 60.23it/s]\n",
      "100%|██████████| 4287/4287 [01:11<00:00, 59.77it/s]\n",
      "100%|██████████| 4287/4287 [01:12<00:00, 59.51it/s]\n",
      "100%|██████████| 4287/4287 [01:12<00:00, 59.12it/s]\n",
      "100%|██████████| 4287/4287 [01:13<00:00, 58.48it/s]\n"
     ]
    }
   ],
   "source": [
    "scrubber, losses = fit_sequential(\n",
    "    model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [01:08<00:00, 62.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9937019348144531 of losses are finite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, loss = sanity_check(\n",
    "    scrubber, model, tokenizer, train_tokens, dataset['train']['upos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import (\n",
    "    binary_cross_entropy_with_logits as bce_with_logits,\n",
    ")\n",
    "from torch.nn.functional import (\n",
    "    cross_entropy,\n",
    ")\n",
    "\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    \"\"\"Linear classifier trained with supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_classes: int = 2,\n",
    "        device: str | torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(\n",
    "            input_dim, num_classes if num_classes > 2 else 1, device=device, dtype=dtype\n",
    "        )\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear(x).squeeze(-1)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def fit(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor,\n",
    "        *,\n",
    "        l2_penalty: float = 0.0,\n",
    "        max_iter: int = 10_000,\n",
    "    ) -> float:\n",
    "        \"\"\"Fits the model to the input data using L-BFGS with L2 regularization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, D), where N is the number of samples and D is\n",
    "                the input dimension.\n",
    "            y: Target tensor of shape (N,) for binary classification or (N, C) for\n",
    "                multiclass classification, where C is the number of classes.\n",
    "            l2_penalty: L2 regularization strength.\n",
    "            max_iter: Maximum number of iterations for the L-BFGS optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Final value of the loss function after optimization.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            self.parameters(),\n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            max_iter=max_iter,\n",
    "        )\n",
    "\n",
    "        num_classes = self.linear.out_features\n",
    "        loss_fn = bce_with_logits if num_classes == 1 else cross_entropy\n",
    "        loss = torch.inf\n",
    "        y = y.to(\n",
    "            torch.get_default_dtype() if num_classes == 1 else torch.long,\n",
    "        )\n",
    "\n",
    "        def closure():\n",
    "            nonlocal loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss function\n",
    "            logits = self(x).squeeze(-1)\n",
    "            loss = loss_fn(logits, y)\n",
    "            if l2_penalty:\n",
    "                reg_loss = loss + l2_penalty * self.linear.weight.square().sum()\n",
    "            else:\n",
    "                reg_loss = loss\n",
    "\n",
    "            reg_loss.backward()\n",
    "            return float(reg_loss)\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        return float(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([h.squeeze(0) for h in x[-1]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(y, dtype=torch.long, device=X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(\n",
    "    x[-1][0].shape[-1], num_classes=len(upos_tags), device=X.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "Y_1h = F.one_hot(Y, len(upos_tags)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3862e-09,  7.2934e-09,  9.7994e-09,  ...,  9.3750e-11,\n",
       "         -6.8982e-09, -8.0569e-09],\n",
       "        [ 4.4910e-11,  1.6019e-08,  1.8144e-09,  ..., -4.6749e-10,\n",
       "         -1.4405e-08, -5.7260e-09],\n",
       "        [ 3.5569e-09, -1.4820e-09, -6.4671e-10,  ...,  4.8699e-11,\n",
       "         -8.9820e-10,  5.1018e-09],\n",
       "        ...,\n",
       "        [-6.5748e-09, -3.5928e-11,  5.6227e-09,  ..., -3.3079e-10,\n",
       "          2.6587e-09, -2.1153e-09],\n",
       "        [-4.3473e-09,  1.2754e-09, -1.2665e-09,  ...,  3.5226e-11,\n",
       "         -4.3114e-10,  7.1856e-11],\n",
       "        [-1.1497e-09,  6.1078e-10, -3.0011e-09,  ...,  2.9093e-10,\n",
       "          2.4790e-09,  1.1317e-09]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcov = (X - X.mean(0)).T @ (Y_1h - Y_1h.mean(0)) / (X.shape[0] - 1)\n",
    "xcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4124042987823486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4123, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.bincount(Y)\n",
    "probs = counts / counts.sum()\n",
    "H = -(probs * probs.add(1e-8).log()).sum()\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean loss: 3.11: 100%|██████████| 1216/1216 [01:42<00:00, 11.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#scrubber.clear_x()\n",
    "scrubber_adapted = deepcopy(scrubber)\n",
    "scrubber_adapted.clear_x()\n",
    "\n",
    "with scrubber_adapted.record(model), torch.no_grad():\n",
    "    clean_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        clean_losses.append(model(x, labels=x).loss)\n",
    "        pbar.set_description(\n",
    "            f\"Clean loss: {torch.stack(clean_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random loss: 3.47: 100%|██████████| 1216/1216 [01:44<00:00, 11.60it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_losses = []\n",
    "    model = model.float()\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber_adapted.random_scrub(model):\n",
    "            random_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Random loss: {torch.stack(random_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random loss: 3.47: 100%|██████████| 1216/1216 [01:45<00:00, 11.57it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_losses = []\n",
    "    model = model.float()\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber.random_scrub(model):\n",
    "            random_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Random loss: {torch.stack(random_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubber.erasers[1].n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrubbed loss: 9.20: 100%|██████████| 1216/1216 [01:45<00:00, 11.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from concept_erasure import ConceptEraser\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    scrubbed_losses = []\n",
    "\n",
    "    pbar = tqdm(test_set)\n",
    "    for record in pbar:\n",
    "        x = record['input_ids'].to(model.device).unsqueeze(0)\n",
    "\n",
    "        with scrubber_adapted.scrub(model):\n",
    "            scrubbed_losses.append(model(x, labels=x).loss)\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Scrubbed loss: {torch.stack(scrubbed_losses).nanmean():.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33238209937056146"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats_per_bpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1662, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8471, device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(clean_losses).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.2455, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(scrubbed_losses).nanmean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
